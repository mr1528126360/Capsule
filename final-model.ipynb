{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(inputs, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param inputs: vectors to be squashed\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same size as inputs\n",
    "    \"\"\"\n",
    "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
    "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
    "    return scale * inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Conv2D with `out_channels` and then reshape to get capsules\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param dim_caps: dimension of capsule\n",
    "    :param kernel_size: kernel size\n",
    "    :return: output tensor, size=[batch, num_caps, dim_caps]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
    "        super(PrimaryCapsule, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.conv2d(x)\n",
    "        outputs = outputs.view(x.size(0), -1, self.dim_caps)\n",
    "        return squash(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    The dense capsule layer. It is similar to Dense (FC) layer. Dense layer has `in_num` inputs, each is a scalar, the\n",
    "    output of the neuron from the former layer, and it has `out_num` output neurons. DenseCapsule just expands the\n",
    "    output of the neuron from scalar to vector. So its input size = [None, in_num_caps, in_dim_caps] and output size = \\\n",
    "    [None, out_num_caps, out_dim_caps]. For Dense Layer, in_dim_caps = out_dim_caps = 1.\n",
    "\n",
    "    :param in_num_caps: number of cpasules inputted to this layer\n",
    "    :param in_dim_caps: dimension of input capsules\n",
    "    :param out_num_caps: number of capsules outputted from this layer\n",
    "    :param out_dim_caps: dimension of output capsules\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
    "        super(DenseCapsule, self).__init__()\n",
    "        self.in_num_caps = in_num_caps\n",
    "        self.in_dim_caps = in_dim_caps\n",
    "        self.out_num_caps = out_num_caps\n",
    "        self.out_dim_caps = out_dim_caps\n",
    "        self.routings = routings\n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = torch.squeeze(torch.matmul(self.weight, x[:, None, :, :, None]), dim=-1)\n",
    "        x_hat_detached = x_hat.detach()\n",
    "\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.size = [batch, out_num_caps, in_num_caps]\n",
    "        b = torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)\n",
    "\n",
    "        assert self.routings > 0, 'The \\'routings\\' should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.size = [batch, out_num_caps, in_num_caps]\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `x_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.routings - 1:\n",
    "                # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
    "                # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => outputs.size=   [batch, out_num_caps, 1,           out_dim_caps]\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat))  # alternative way\n",
    "            else:  # Otherwise, use `x_hat_detached` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat_detached))  # alternative way\n",
    "\n",
    "                # outputs.size       =[batch, out_num_caps, 1,           out_dim_caps]\n",
    "                # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => b.size          =[batch, out_num_caps, in_num_caps]\n",
    "                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n",
    "\n",
    "        return torch.squeeze(outputs, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNet_1(nn.Module):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_size: data size = [channels, width, height]\n",
    "    :param classes: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    Shape:\n",
    "        - Input: (batch, channels, width, height), optional (batch, classes) .\n",
    "        - Output:((batch, classes), (batch, channels, width, height))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, classes, routings):\n",
    "        super(CapsuleNet_1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.routings = routings\n",
    "\n",
    "        # Layer 1: Just a conventional Conv2D layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_size[0], 256, kernel_size=9, stride=1, padding=0)\n",
    "\n",
    "        # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps]\n",
    "        self.primarycaps = PrimaryCapsule(\n",
    "            256, 256, 8, kernel_size=9, stride=2, padding=0)\n",
    "\n",
    "        # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "        #self.digitcaps = DenseCapsule(in_num_caps=32*6*6, in_dim_caps=8,\n",
    "        #                              out_num_caps=classes, out_dim_caps=16, routings=routings)\n",
    "        self.digitcaps = DenseCapsule(in_num_caps=1486848, in_dim_caps=8,\n",
    "                                      out_num_caps=classes, out_dim_caps=25, routings=routings)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(50,2)\n",
    "        #self.fc2 = nn.Linear(10,2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primarycaps(x)\n",
    "        x = self.digitcaps(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=2)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        #x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #length = x.norm(dim=-1)\n",
    "        #print(length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    b = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "                c = (net(X).argmax(dim=1)).numpy()\n",
    "                b.extend(c)\n",
    "                #print(b)\n",
    "                #net.train() # 改回训练模式\n",
    "            else: \n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()\n",
    "                    result = (net(X, is_training=False).argmax(dim=1)).float().sum().item()\n",
    "                    c = (net(X).argmax(dim=1)).numpy()\n",
    "                    b.extend(c)\n",
    "                    #print(b)\n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "                    result = (net(X).argmax(dim=1)).float().sum().item()\n",
    "                    c = (net(X).argmax(dim=1)).numpy()\n",
    "                    b.extend(c)\n",
    "                    #print(b)\n",
    "            n += y.shape[0]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNet_2(nn.Module):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_size: data size = [channels, width, height]\n",
    "    :param classes: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    Shape:\n",
    "        - Input: (batch, channels, width, height), optional (batch, classes) .\n",
    "        - Output:((batch, classes), (batch, channels, width, height))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, classes, routings):\n",
    "        super(CapsuleNet_2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.routings = routings\n",
    "\n",
    "        # Layer 1: Just a conventional Conv2D layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_size[0], 256, kernel_size=9, stride=1, padding=0)\n",
    "\n",
    "        # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps]\n",
    "        self.primarycaps = PrimaryCapsule(\n",
    "            256, 256, 8, kernel_size=9, stride=2, padding=0)\n",
    "\n",
    "        # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "        #self.digitcaps = DenseCapsule(in_num_caps=32*6*6, in_dim_caps=8,\n",
    "        #                              out_num_caps=classes, out_dim_caps=16, routings=routings)\n",
    "        self.digitcaps = DenseCapsule(in_num_caps=1486848, in_dim_caps=8,\n",
    "                                      out_num_caps=classes, out_dim_caps=20, routings=routings)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(40,10)\n",
    "        self.fc2 = nn.Linear(10,2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primarycaps(x)\n",
    "        x = self.digitcaps(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=2)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #length = x.norm(dim=-1)\n",
    "        #print(length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "data_1 = pd.read_csv(\"naocuzchuli1.csv\",sep=\",\", engine='python', encoding='utf-8')\n",
    "data = pd.read_csv(\"数据标注.csv\",sep=\",\", engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findname(name):\n",
    "    for i in range(0,len(name)):\n",
    "        #print(name[i][0])\n",
    "        if(name[i][0] == '2' and name[i][1] == '-'):\n",
    "            file_dir1 = file_dir+\"/\"+name[i]\n",
    "            break\n",
    "        else:\n",
    "            if(name[i][0] == '5'and name[i][1] == '-'):\n",
    "                file_dir1 = file_dir+\"/\"+name[i]\n",
    "                #print(file_dir1)\n",
    "                break\n",
    "            else:\n",
    "                if(name[i][0] == '6'and name[i][1] == '-'):\n",
    "                    file_dir1 = file_dir+\"/\"+name[i]\n",
    "                    break\n",
    "                else:\n",
    "                    if(name[i][0] == '3'and name[i][1] == '-'):\n",
    "                        file_dir1 = file_dir+\"/\"+name[i]\n",
    "                        break\n",
    "                    else:\n",
    "                        if(name[i][0] == '9'and name[i][1] == '-'):\n",
    "                            file_dir1 = file_dir+\"/\"+name[i]\n",
    "                            break\n",
    "    return file_dir1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_n = 100\n",
    "def readimage(file_dir,name,size_n):\n",
    "    character_address = file_dir+\"/\"+name\n",
    "    #print(character_address)\n",
    "    d = cv2.imread(character_address,0)\n",
    "    d = cv2.resize(d, (size_n, size_n))\n",
    "    return d\n",
    "#d = readimage(file_dir1,name_1[0],size_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MR201802210163-Wu HuaHao\n",
      "./MR201706120215-LiangYuQing\n",
      "./MR201602290243-Liang QiPan\n",
      "./MR201803090487-HuangTianDi\n",
      "./MR201710230147-HuangJiangQuan\n",
      "./MR2017062901390-LinGen You\n",
      "./MR201707060266-ChenGuiYing\n",
      "./MR201806250211-HuangHuiZhong\n",
      "./MR201806040279-Li JinRong\n",
      "./MR202103160080-Chen LianDing\n",
      "./MR201807080094-He Zhong\n",
      "./MR201708080222-Li GuiFeng\n",
      "./MR202109160378-Liang YouDi\n",
      "./MR201802200016-ZhaoDongSheng\n",
      "./MR201803140073-Fang LiChang\n",
      "./MR201711160167-Huang ZhiRong\n",
      "./MR201709170104-LiangSuZhen\n",
      "./MR202110280279-Yang XiTian\n",
      "./MR201803170300-Huang GenFa\n",
      "./MR201805060354-XieShengYu\n",
      "./MR201812090066-Lin Nong\n",
      "./MR201810030079-Huang QiDing\n",
      "./MR201806070184-ZhouLiBo\n",
      "./MR201811040118-Pan YiYou\n",
      "./MR202204270343-Huang BingQiang\n",
      "./MR201807130152-Zhang SheXie\n",
      "./MR201901140071-Xiao BaoXi\n",
      "./MR202007040281-Yin CaiDi\n",
      "./MR201907170393-Ou FuTai\n",
      "./MR201808280243-Lin ShiMei\n",
      "./MR201808150286-Huang ZhengLian\n",
      "./MR201902140144-Zhou RuiQiu\n",
      "./MR201905270177-Lu SongMao\n",
      "./MR201908300389-Huang YanChun\n",
      "./MR201909240217-Huang XianYou\n",
      "./MR201809280461-Huang GuiPing\n",
      "./MR201908140375-Huang ShunAn\n",
      "./MR201901010047-Zhao PanShun\n",
      "./MR202203140317-Huang MingZhong\n",
      "./MR201906120420-Zhang YanHua\n",
      "./MR202204250318-Yang NuLan\n",
      "./MR201811220371-Kuang JianHua\n",
      "./MR201811010032-Zhao QiSheng\n",
      "./MR201811170295-Kuang GuoHong\n",
      "./MR201911230367-Zhou QunDuo\n",
      "./MR202011170463-Zhou YunNu\n",
      "./MR201812290239-Zhang ZhuoSen\n",
      "./MR201906110314-Song YuCheng\n",
      "./MR201901190185-Li DuSheng\n",
      "./MR202111040024-Lin GuiMei\n",
      "./MR202004090247-Liang DieRong\n",
      "./MR201903030032-Zhou WenXing\n",
      "./MR201910280291-LiLinYang\n",
      "./MR201905210129-Peng DaZe\n",
      "./MR201910010051-Zeng ShunFa\n",
      "./MR201904300236-He ChangLian\n",
      "./MR202012140342-Liang MeiShun\n",
      "./MR202103270245-Zhou YunHao\n",
      "./MR201909300333-Wu HuanLin\n",
      "./MR201905280271-Zhang RenDe\n",
      "./MR201911230290-Guo FengMan\n",
      "./MR-Pan BaoCai\n",
      "./MR201709050086-Mei BenZhou\n",
      "./MR201709060196-XieJian\n",
      "./MR201709050212-Hu ShengYin\n",
      "./MR202003070164-He QuanHuan\n",
      "./MR201709100131-LiangJinDi\n",
      "./MR201709120100-LiangJinYuan\n",
      "./MR201709080242-ZhuYuanTian\n",
      "./MR201709110124-Zhou JianLe\n",
      "./MR201709150146-He LanZhong\n",
      "./MR201709180125-ChenShaoLing\n",
      "./MR201709130216-LuNiuXi\n",
      "./MR201709180311-LuoSan\n",
      "./MR201709170194-WuChangYang\n",
      "./MR201709250187-WeiLongXi\n",
      "./MR201709250264-Zhang HuanJiao\n",
      "./MR201709240112-LiaoSheSheng\n",
      "./MR201709250212-OuYangLiJuan\n",
      "./MR201709260229-Zhang MinHui\n",
      "./MR201709290106-Liang DaFu\n",
      "./MR201710030170-WangPeiGen\n",
      "./MR201609020118-Zhou Wei\n",
      "./MR201710080077-ZengGuoCai\n",
      "./MR201710260143-LuZhi\n",
      "./MR201710230298-LiangYunHao\n",
      "./MR201710280069-HuangJiHan\n",
      "./MR201710290080-ZhuJianSheng\n",
      "./MR201711010144-ChenZhaoGuang\n",
      "./MR201803070303-Mo JinMei\n",
      "./MR201711040248-YangFengJiao\n",
      "./MR201711090081-ZouSanMei\n",
      "./MR201704290123-RenZiFeng\n",
      "./MR201711080214-LiangBaiSheng\n",
      "./MR201710190322-ZhaoDongQiang\n",
      "./MR201711070313-Zhao YeQin\n",
      "./MR201811160271-Li ZhaoQun\n",
      "./MR201711200279-RongGuangNing\n",
      "./MR201711220152-He MeiQun\n",
      "./MR201711240273-ChenSiReng\n",
      "./MR201711180209-GuoZhenQian\n",
      "./MR201709100154-Lin ZhenQuan\n",
      "./MR201711150153-KuangXingYing\n",
      "./MR201711300156-LeFaZi\n",
      "./MR201712010104-Chen EnHua\n",
      "./MR201711290250-YaoYanRong\n",
      "./MR201904240211-Wu YouGen\n",
      "./MR201910070088-Ou DaYing\n",
      "./MR201712010181-Liao LianZhong\n",
      "./MR201712050082-FengZaiYing\n",
      "./MR201709240102-Lao NuYong\n",
      "./MR201712120209-Kuang YanPing\n",
      "./MR201712170069-ZhangSanDing\n",
      "./MR201712180097-Zhao LeSheng\n",
      "./MR201712190182-WuXuShan\n",
      "./MR201712220318-MaGuiXiang\n",
      "./MR201712170079-WuWeiHuan\n",
      "./MR201712210191-DengXiuJuan\n",
      "./MR201712210375-XuQiNu\n",
      "./MR201712240170-Liu QingXiao\n",
      "./MR201712250442-Luo FuJin\n",
      "./MR201712260308-ZhouJunQiang\n",
      "./MR201712190202-ChenKunZhen\n",
      "./MR201712260400-Liang JinKuan\n",
      "./MR201712220354-LuoHuaJin\n",
      "./MR201712310040-HuangYanZhuo\n",
      "./MR201712260397-Huang GuoDong\n",
      "./MR201801030282-LiChengQing\n",
      "./MR201712260409-HuangYingYou\n",
      "./MR201801270027-KuangGuoCai\n",
      "./MR201805130099-PanBaoLiang\n",
      "./MR201902120162-Zhang Cong\n",
      "./MR201810280134-Rong MeiHuan\n",
      "./MR201901181085-Hu YunLan\n",
      "./MR201712310157-Zhou BaiSen\n",
      "./MR201608140234-Liang JianRong\n",
      "./MR201801130182-XuShengQuan\n",
      "./MR201801060242-Wang SuZhen\n",
      "./MR201908220397-Zhang GuoYin\n",
      "./MR201801070152-ZhuShuiZhen\n",
      "./MR201801040143-LiuCuiHua\n",
      "./MR201802100202-KuangLiQin\n",
      "./MR-Huang XinPei\n",
      "./MR201801100184-KuangZhuMing\n",
      "./MR201801250133-GanHongYing\n",
      "./MR201801220266-ChenJianJun\n",
      "./MR201801270108-Kuang ChunMei\n",
      "./MR201801280085-ZhangChengJing\n",
      "./MR201807100283-WuYuLing\n",
      "./MR201802180035-ChenGuiQuan\n",
      "./MR201910260239-Huang BaoSheng\n",
      "./MR201802060103-YeChangYan\n",
      "./MR201801260243-DengDongYi\n",
      "./MR201802010201-HuangChengGang\n",
      "./MR201811290275-Zhang MingMin\n",
      "./MR201802120177-Huang ZhongTing\n",
      "./MR201808150161-Sun Jing\n",
      "./MR201803010795-HuangNuChang\n",
      "./MR201802260271-WangXiuZhi\n",
      "./MR201812140342-Lai ShuiYin\n",
      "./MR201803080295-Liang JinPei\n",
      "./MR201803080324-WuSongHua\n",
      "./MR201803080017-LiuJian\n",
      "./MR201803120307-Liu XiuLian\n",
      "./MR201811230369-Luo PingMei\n",
      "./MR201803080200-HuQunYing\n",
      "./MR201803120328-ZhangXingLai\n",
      "./MR201803150112-LiangChuanLiu\n",
      "./MR201803120367-Zhou XiuNong\n",
      "./MR201803140023-ChenYuHong\n",
      "./MR201804220167-FengMei\n",
      "./MR202106100062-Zhou XianMei\n",
      "./MR201803300207-LuoXiaNu\n",
      "./MR201812120201-Kuang YuXiong\n",
      "./MR201803290560-ChenJianFang\n",
      "./MR201804040022-Chen NianJin\n",
      "./MR201803290614-HuangLe\n",
      "./MR201708110264-LvYueQing\n",
      "./MR201804130221-He WeiYuan\n",
      "./MR201804090321-LiGuangMing\n",
      "./MR201804130132-WuChangMei\n",
      "./MR201808060137-Yang XianYing\n",
      "./MR201903120286-Duan ShaoPing\n",
      "./MR201804140320-HuangYuZhen\n",
      "./MR201804200348-YiJiYuan\n",
      "./MR201804170212-ChenWangMing\n",
      "./MR201903010218-Lv RuiYing\n",
      "./MR201804230415-LinZhenHong\n",
      "./MR201804130045-WangLing\n",
      "./MR201804200221-LinXiuRong\n",
      "./MR201804210230-WenJingCheng\n",
      "./MR201709140208-HuangZuoSong\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "X_data = []\n",
    "path_label=[]\n",
    "for h in range(0,len(data[\"ID\"])):\n",
    "    file_dir='./'+data[\"ID\"][h]\n",
    "    print(file_dir)\n",
    "    # ⽬录下⾯的所有⽂件名\n",
    "    name = os.listdir(file_dir)\n",
    "    file_dir2 = findname(name)\n",
    "    name_1 = os.listdir(file_dir2)\n",
    "    #print(h)\n",
    "    #data.append\n",
    "    #print(name_1)\n",
    "    #print(file_dir2)\n",
    "    for i in range(0,20):\n",
    "        if i == 0:\n",
    "            #print(file_dir2)\n",
    "            d = readimage(file_dir2,name_1[i],size_n)\n",
    "        else:\n",
    "            c = readimage(file_dir2,name_1[i],size_n)\n",
    "            d = np.hstack((d,c))\n",
    "    #print(d.shape)\n",
    "    d = np.reshape(d, (400, 500))\n",
    "    X_data.append(d)\n",
    "    path_label.append(data[\"Lable\"][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 1, 400, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_1 = np.array(X_data)\n",
    "X_data_1 = X_data_1.reshape(X_data_1.shape[0],1 ,X_data_1.shape[1], X_data_1.shape[2])\n",
    "X_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "list_1 = []\n",
    "for i in range(0,len(data)):\n",
    "    for j in range(0,len(data_1)):\n",
    "        if data[\"姓名\"][i]==data_1[\"姓名\"][j]:\n",
    "            list.append(j)\n",
    "            list_1.append(data[\"Lable\"][i])\n",
    "dataset = data_1.iloc[list]\n",
    "#dataset = dataset.drop([\"姓名\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_1 = data_1.iloc[list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "#categorical_labels = to_categorical(y_test, num_classes=2)\n",
    "#print(categorical_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, list_1, test_size=0.2)\n",
    "#Xtrain = np.array(X_train)\n",
    "#Xtext = np.array(X_test)\n",
    "#y_train = np.array(y_train)\n",
    "#y_test = np.array(y_test)\n",
    "#y_train = to_categorical(y_train, num_classes=2)\n",
    "#y_test = to_categorical(y_test, num_classes=2)\n",
    "#print(Xtrain.shape)\n",
    "#print(Xtext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list3 = []\n",
    "for i in range(0,len(X_test)):\n",
    "    #print(X_test[\"姓名\"][X_test.index[i]])\n",
    "    for j in range(0,len(data)):\n",
    "        if(data[\"姓名\"][j] == X_test[\"姓名\"][X_test.index[i]]):\n",
    "            list3.append(j)\n",
    "#list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_DL = X_data_1[list3]\n",
    "X_train = X_train.drop([\"姓名\"],axis=1)\n",
    "X_test = X_test.drop([\"姓名\"],axis=1)\n",
    "Xtrain = np.array(X_train)\n",
    "Xtext = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>性别</th>\n",
       "      <th>年龄</th>\n",
       "      <th>住院时间</th>\n",
       "      <th>吸烟</th>\n",
       "      <th>饮酒</th>\n",
       "      <th>消化性溃疡史</th>\n",
       "      <th>TIA/卒中史</th>\n",
       "      <th>糖尿病</th>\n",
       "      <th>高血压分级</th>\n",
       "      <th>房颤</th>\n",
       "      <th>...</th>\n",
       "      <th>C反应蛋白(mg/l)</th>\n",
       "      <th>血清白蛋白水平(g/l)</th>\n",
       "      <th>载脂蛋白a1(g/l)</th>\n",
       "      <th>载脂蛋白b</th>\n",
       "      <th>血小板水平（*109）</th>\n",
       "      <th>白细胞水平（*109）</th>\n",
       "      <th>中性粒细胞百分比</th>\n",
       "      <th>纤维蛋白原</th>\n",
       "      <th>糖化血红蛋白</th>\n",
       "      <th>颈动脉粥样硬化</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>40.4</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>241</td>\n",
       "      <td>6.8</td>\n",
       "      <td>49.9</td>\n",
       "      <td>2.88</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.72</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.04</td>\n",
       "      <td>264</td>\n",
       "      <td>9.3</td>\n",
       "      <td>76.4</td>\n",
       "      <td>3.02</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.09</td>\n",
       "      <td>250</td>\n",
       "      <td>7.9</td>\n",
       "      <td>84.6</td>\n",
       "      <td>3.31</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.22</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.88</td>\n",
       "      <td>200</td>\n",
       "      <td>6.4</td>\n",
       "      <td>58.3</td>\n",
       "      <td>2.92</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>41.6</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.96</td>\n",
       "      <td>256</td>\n",
       "      <td>8.4</td>\n",
       "      <td>68.3</td>\n",
       "      <td>2.28</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.34</td>\n",
       "      <td>46.7</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.84</td>\n",
       "      <td>310</td>\n",
       "      <td>10.5</td>\n",
       "      <td>68.9</td>\n",
       "      <td>3.48</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>40.3</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.37</td>\n",
       "      <td>277</td>\n",
       "      <td>6.4</td>\n",
       "      <td>66.5</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.07</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.76</td>\n",
       "      <td>204</td>\n",
       "      <td>5.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>3.87</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.27</td>\n",
       "      <td>41.3</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.12</td>\n",
       "      <td>225</td>\n",
       "      <td>7.8</td>\n",
       "      <td>67.5</td>\n",
       "      <td>3.55</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.42</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.41</td>\n",
       "      <td>217</td>\n",
       "      <td>9.8</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.02</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.26</td>\n",
       "      <td>42.7</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.26</td>\n",
       "      <td>256</td>\n",
       "      <td>5.3</td>\n",
       "      <td>47.7</td>\n",
       "      <td>2.91</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.42</td>\n",
       "      <td>37.2</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.82</td>\n",
       "      <td>208</td>\n",
       "      <td>5.9</td>\n",
       "      <td>59.8</td>\n",
       "      <td>2.26</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.97</td>\n",
       "      <td>34.9</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.19</td>\n",
       "      <td>149</td>\n",
       "      <td>6.5</td>\n",
       "      <td>76.4</td>\n",
       "      <td>3.96</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.07</td>\n",
       "      <td>42.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.19</td>\n",
       "      <td>202</td>\n",
       "      <td>6.8</td>\n",
       "      <td>71.6</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.05</td>\n",
       "      <td>44.7</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.07</td>\n",
       "      <td>251</td>\n",
       "      <td>5.3</td>\n",
       "      <td>85.3</td>\n",
       "      <td>2.83</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.97</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.16</td>\n",
       "      <td>242</td>\n",
       "      <td>11.4</td>\n",
       "      <td>83.9</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.19</td>\n",
       "      <td>42.3</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.07</td>\n",
       "      <td>270</td>\n",
       "      <td>7.0</td>\n",
       "      <td>72.8</td>\n",
       "      <td>3.45</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.40</td>\n",
       "      <td>30.8</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>301</td>\n",
       "      <td>6.5</td>\n",
       "      <td>54.7</td>\n",
       "      <td>3.64</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>52.2</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.05</td>\n",
       "      <td>308</td>\n",
       "      <td>10.6</td>\n",
       "      <td>88.4</td>\n",
       "      <td>3.69</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.70</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.45</td>\n",
       "      <td>223</td>\n",
       "      <td>15.4</td>\n",
       "      <td>88.6</td>\n",
       "      <td>4.58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.17</td>\n",
       "      <td>37.2</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.04</td>\n",
       "      <td>221</td>\n",
       "      <td>6.9</td>\n",
       "      <td>68.4</td>\n",
       "      <td>3.80</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.21</td>\n",
       "      <td>31.8</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>441</td>\n",
       "      <td>12.7</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.67</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.40</td>\n",
       "      <td>40.7</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.39</td>\n",
       "      <td>244</td>\n",
       "      <td>5.3</td>\n",
       "      <td>59.8</td>\n",
       "      <td>3.67</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.95</td>\n",
       "      <td>37.8</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.05</td>\n",
       "      <td>199</td>\n",
       "      <td>7.4</td>\n",
       "      <td>80.7</td>\n",
       "      <td>3.32</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>45.4</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.16</td>\n",
       "      <td>231</td>\n",
       "      <td>4.6</td>\n",
       "      <td>55.6</td>\n",
       "      <td>3.05</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>37.1</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.67</td>\n",
       "      <td>149</td>\n",
       "      <td>6.2</td>\n",
       "      <td>54.7</td>\n",
       "      <td>3.12</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>14.74</td>\n",
       "      <td>40.4</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.61</td>\n",
       "      <td>160</td>\n",
       "      <td>8.8</td>\n",
       "      <td>71.4</td>\n",
       "      <td>3.52</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.61</td>\n",
       "      <td>33.9</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.95</td>\n",
       "      <td>194</td>\n",
       "      <td>5.4</td>\n",
       "      <td>59.1</td>\n",
       "      <td>3.87</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.86</td>\n",
       "      <td>242</td>\n",
       "      <td>6.6</td>\n",
       "      <td>52.2</td>\n",
       "      <td>2.37</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>41.6</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.05</td>\n",
       "      <td>231</td>\n",
       "      <td>6.6</td>\n",
       "      <td>67.7</td>\n",
       "      <td>3.21</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.35</td>\n",
       "      <td>41.9</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.27</td>\n",
       "      <td>231</td>\n",
       "      <td>5.6</td>\n",
       "      <td>65.1</td>\n",
       "      <td>3.44</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.26</td>\n",
       "      <td>38.9</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.35</td>\n",
       "      <td>304</td>\n",
       "      <td>12.1</td>\n",
       "      <td>49.7</td>\n",
       "      <td>2.19</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.90</td>\n",
       "      <td>40.5</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.81</td>\n",
       "      <td>258</td>\n",
       "      <td>9.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>40.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.71</td>\n",
       "      <td>226</td>\n",
       "      <td>6.9</td>\n",
       "      <td>63.3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.28</td>\n",
       "      <td>44.8</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.09</td>\n",
       "      <td>314</td>\n",
       "      <td>10.8</td>\n",
       "      <td>78.3</td>\n",
       "      <td>2.93</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.51</td>\n",
       "      <td>46.1</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.13</td>\n",
       "      <td>245</td>\n",
       "      <td>9.8</td>\n",
       "      <td>61.5</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.22</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.73</td>\n",
       "      <td>266</td>\n",
       "      <td>8.2</td>\n",
       "      <td>75.1</td>\n",
       "      <td>3.25</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.99</td>\n",
       "      <td>38.2</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.74</td>\n",
       "      <td>265</td>\n",
       "      <td>8.8</td>\n",
       "      <td>80.6</td>\n",
       "      <td>3.07</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>211</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.7</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     性别  年龄  住院时间  吸烟  饮酒  消化性溃疡史  TIA/卒中史  糖尿病  高血压分级  房颤  ...  C反应蛋白(mg/l)  \\\n",
       "564   0  59     7   1   3       0        0    0      3   0  ...         0.56   \n",
       "170   0  52     3   0   2       0        0    0      3   0  ...         1.72   \n",
       "169   0  71     5   0   0       0        0    0      3   0  ...         0.48   \n",
       "199   0  65     8   0   0       0        0    0      3   0  ...         1.22   \n",
       "205   0  51     6   1   3       0        0    0      1   0  ...         0.73   \n",
       "165   0  60     6   0   2       0        0    1      3   0  ...         6.34   \n",
       "504   1  74     6   0   0       1        0    0      3   0  ...         0.86   \n",
       "553   0  72     9   1   1       0        0    1      3   0  ...        72.07   \n",
       "146   1  66     5   0   0       0        0    0      3   0  ...         9.27   \n",
       "286   1  77     6   0   0       0        0    0      1   0  ...        10.42   \n",
       "161   1  78     6   0   0       0        0    0      3   0  ...         1.26   \n",
       "273   0  80     7   0   0       0        0    0      3   0  ...         2.42   \n",
       "284   0  83     5   1   1       0        0    0      0   0  ...        33.97   \n",
       "481   0  53     7   0   0       0        0    0      3   0  ...         5.07   \n",
       "247   1  46     8   0   0       0        0    0      0   0  ...         2.05   \n",
       "190   1  71     9   0   0       0        0    0      3   0  ...         3.97   \n",
       "209   1  70    12   0   0       0        0    0      3   0  ...         6.19   \n",
       "259   0  69     8   0   0       0        0    0      2   0  ...        42.40   \n",
       "195   0  59     7   0   0       0        0    0      3   0  ...         5.00   \n",
       "108   1  69     4   0   0       0        0    0      2   0  ...        40.70   \n",
       "66    1  86     7   0   0       0        0    0      2   0  ...        16.17   \n",
       "211   0  47    21   0   0       0        0    0      0   0  ...         7.21   \n",
       "162   0  80     4   0   0       0        0    0      3   0  ...         1.40   \n",
       "261   1  87    14   0   0       0        0    0      3   0  ...        10.95   \n",
       "221   1  48     4   0   0       0        0    0      3   0  ...         2.25   \n",
       "530   0  74     6   1   1       1        0    0      3   0  ...         0.95   \n",
       "85    1  70    11   1   0       0        0    0      3   1  ...        14.74   \n",
       "383   0  85     5   0   0       0        0    1      2   0  ...         3.61   \n",
       "194   0  76    12   0   0       0        0    0      3   0  ...         0.72   \n",
       "589   1  74     3   0   0       0        0    0      3   0  ...         1.04   \n",
       "239   1  64     9   0   0       0        0    1      2   0  ...         5.35   \n",
       "360   0  64     9   1   0       0        0    0      3   0  ...         1.26   \n",
       "227   0  80    17   0   0       1        0    0      3   0  ...         4.90   \n",
       "384   0  68     8   0   0       0        0    1      3   0  ...         0.75   \n",
       "516   1  68    11   0   0       0        0    1      2   1  ...         1.28   \n",
       "99    1  60     6   0   0       0        0    1      0   0  ...         6.51   \n",
       "207   1  83    11   0   0       0        0    0      3   0  ...         1.22   \n",
       "275   1  79    29   0   0       0        0    0      3   1  ...         5.99   \n",
       "163   0  43     9   1   0       0        0    0      0   0  ...         0.07   \n",
       "\n",
       "     血清白蛋白水平(g/l)  载脂蛋白a1(g/l)  载脂蛋白b  血小板水平（*109）  白细胞水平（*109）  中性粒细胞百分比  \\\n",
       "564          40.4         0.98   0.93          241          6.8      49.9   \n",
       "170          47.0         1.63   1.04          264          9.3      76.4   \n",
       "169          41.0         1.20   1.09          250          7.9      84.6   \n",
       "199          38.5         0.98   0.88          200          6.4      58.3   \n",
       "205          41.6         1.22   0.96          256          8.4      68.3   \n",
       "165          46.7         1.21   0.84          310         10.5      68.9   \n",
       "504          40.3         1.33   1.37          277          6.4      66.5   \n",
       "553          37.5         0.87   0.76          204          5.0      70.5   \n",
       "146          41.3         1.15   1.12          225          7.8      67.5   \n",
       "286          39.2         1.08   1.41          217          9.8      61.8   \n",
       "161          42.7         1.18   1.26          256          5.3      47.7   \n",
       "273          37.2         0.89   0.82          208          5.9      59.8   \n",
       "284          34.9         1.20   1.19          149          6.5      76.4   \n",
       "481          42.8         1.00   1.19          202          6.8      71.6   \n",
       "247          44.7         1.23   1.07          251          5.3      85.3   \n",
       "190          39.2         1.02   1.16          242         11.4      83.9   \n",
       "209          42.3         1.29   1.07          270          7.0      72.8   \n",
       "259          30.8         1.10   1.05          301          6.5      54.7   \n",
       "195          52.2         1.07   1.05          308         10.6      88.4   \n",
       "108          38.5         0.80   0.45          223         15.4      88.6   \n",
       "66           37.2         1.09   1.04          221          6.9      68.4   \n",
       "211          31.8         1.25   0.72          441         12.7      67.0   \n",
       "162          40.7         1.06   0.39          244          5.3      59.8   \n",
       "261          37.8         0.74   1.05          199          7.4      80.7   \n",
       "221          45.4         1.05   1.16          231          4.6      55.6   \n",
       "530          37.1         1.05   0.67          149          6.2      54.7   \n",
       "85           40.4         1.47   1.61          160          8.8      71.4   \n",
       "383          33.9         0.86   0.95          194          5.4      59.1   \n",
       "194          36.7         0.94   0.86          242          6.6      52.2   \n",
       "589          41.6         1.23   1.05          231          6.6      67.7   \n",
       "239          41.9         0.87   1.27          231          5.6      65.1   \n",
       "360          38.9         1.08   1.35          304         12.1      49.7   \n",
       "227          40.5         1.08   0.81          258          9.0      90.5   \n",
       "384          40.5         0.99   0.71          226          6.9      63.3   \n",
       "516          44.8         1.16   1.09          314         10.8      78.3   \n",
       "99           46.1         1.46   1.13          245          9.8      61.5   \n",
       "207          42.0         1.34   0.73          266          8.2      75.1   \n",
       "275          38.2         1.09   0.74          265          8.8      80.6   \n",
       "163          34.7         0.81   0.82          211          7.0      43.7   \n",
       "\n",
       "     纤维蛋白原  糖化血红蛋白  颈动脉粥样硬化  \n",
       "564   2.88     6.0        1  \n",
       "170   3.02     5.3        0  \n",
       "169   3.31     5.2        1  \n",
       "199   2.92     5.7        1  \n",
       "205   2.28     5.9        1  \n",
       "165   3.48     6.9        0  \n",
       "504   2.81     5.9        1  \n",
       "553   3.87    11.7        1  \n",
       "146   3.55     6.2        1  \n",
       "286   4.02     6.3        1  \n",
       "161   2.91     6.1        1  \n",
       "273   2.26     6.5        1  \n",
       "284   3.96     5.4        1  \n",
       "481   5.41     5.8        1  \n",
       "247   2.83     5.9        0  \n",
       "190   3.24     5.8        1  \n",
       "209   3.45     5.4        1  \n",
       "259   3.64     6.2        1  \n",
       "195   3.69     5.7        1  \n",
       "108   4.58     6.0        1  \n",
       "66    3.80     6.0        1  \n",
       "211   4.67     6.0        0  \n",
       "162   3.67     6.5        1  \n",
       "261   3.32     5.3        1  \n",
       "221   3.05     5.7        1  \n",
       "530   3.12     5.7        1  \n",
       "85    3.52     7.0        1  \n",
       "383   3.87     6.7        1  \n",
       "194   2.37     6.0        1  \n",
       "589   3.21     5.7        1  \n",
       "239   3.44     6.1        1  \n",
       "360   2.19     6.7        1  \n",
       "227   3.25     4.8        1  \n",
       "384   2.65     6.9        1  \n",
       "516   2.93     6.1        1  \n",
       "99    2.96     7.3        0  \n",
       "207   3.25     5.1        1  \n",
       "275   3.07     5.7        1  \n",
       "163   2.38     5.7        0  \n",
       "\n",
       "[39 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training in 8.35 seconds\n",
      "[1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
      " 1 0]\n",
      "[1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0\n",
      " 0 0]\n",
      "26.0  Correct in total :  39.0 \t 66.66666666666666 %\n",
      "0.6760626066315556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-940c5b2af303>:40: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(mean_fpr, fpr, tpr) #对mean_tpr在mean_fpr处进行插值，通过scipy包调用interp()函数\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.15890911e-01  2.72292434e-02  7.73760760e-02  8.92132371e-01\n",
      "   4.08116139e-01 -6.51655520e-01  0.00000000e+00 -1.03245903e+00\n",
      "   1.90703439e-01  8.83282844e-01  1.00368772e+00 -7.50699658e-03\n",
      "  -5.75222007e-03 -6.16760067e-02 -1.57409134e-02  2.91117219e-03\n",
      "   5.82819593e-01 -1.22045584e+00  3.64955727e-02  1.09327140e+00\n",
      "  -3.25297756e-02  1.54990121e-03 -5.20077396e-02  7.30066096e-01\n",
      "   5.04863031e-01  4.96118649e-03 -1.74574238e-01  4.28385602e-05\n",
      "  -3.06106092e-01  4.02506584e-01  6.08309734e-01]]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np  \n",
    "from scipy import interp  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "  \n",
    "from sklearn import svm, datasets  \n",
    "from sklearn.metrics import roc_curve, auc  \n",
    "#from sklearn.cross_validation import StratifiedKFold  \n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = timeit.default_timer()\n",
    "k = end_time - start_time\n",
    "print(\"training in %s seconds\" % round(end_time - start_time,2))\n",
    "count = 0.0;\n",
    "crct = 0.0\n",
    "start_time = timeit.default_timer()\n",
    "print (clf.predict(X_test))\n",
    "#print clf.predict_proba(test[:,:-1]) \n",
    "#print clf.predict_proba(test[:,:-1])\n",
    "print (y_test)\n",
    "res = clf.predict(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    count += 1\n",
    "    if res[i] == y_test[i]:\n",
    "        crct += 1\n",
    "end_time = timeit.default_timer()\n",
    "id = 15\n",
    "time = k + (end_time - start_time)\n",
    "number = crct/count\n",
    "\n",
    "print (crct,' Correct in total : ',count,'\\t',crct/count * 100,'%')\n",
    "mean_tpr = 0.0  \n",
    "mean_fpr = np.linspace(0, 1, 100)  \n",
    "all_tpr = [] \n",
    "probas_ = clf.predict_proba(X_test) \n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])  \n",
    "mean_tpr += interp(mean_fpr, fpr, tpr) #对mean_tpr在mean_fpr处进行插值，通过scipy包调用interp()函数  \n",
    "mean_tpr[0] = 0.0  #初始处为0  \n",
    "roc_auc = auc(fpr, tpr)\n",
    "print (metrics.f1_score(y_test, probas_[:, 1].round(), average='weighted'))  \n",
    "#画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来  \n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc)) \n",
    "plt.xlim([-0.05, 1.05])  \n",
    "plt.ylim([-0.05, 1.05])  \n",
    "plt.xlabel('False Positive Rate')  \n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('SVM')  \n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show() \n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = torch.utils.data.TensorDataset(torch.FloatTensor(X_test_DL), torch.FloatTensor(y_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(save_name, optimizer, model):\n",
    "    model_data = torch.load(save_name)\n",
    "    model.load_state_dict(model_data['model_dict'])\n",
    "    optimizer.load_state_dict(model_data['optimizer_dict'])\n",
    "    print(\"model load success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load success\n"
     ]
    }
   ],
   "source": [
    "path = \"./model_image_1/model_1.pkl\"\n",
    "batch_size, lr, num_epochs = 5, 0.001, 10\n",
    "net = CapsuleNet_1(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#loss = nn.CrossEntropyLoss()\n",
    "load_model(path, optimizer, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_result = evaluate_accuracy(testloader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load success\n"
     ]
    }
   ],
   "source": [
    "path = \"./model_image_2/model_1.pkl\"\n",
    "batch_size, lr, num_epochs = 5, 0.001, 10\n",
    "net = CapsuleNet_2(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#loss = nn.CrossEntropyLoss()\n",
    "load_model(path, optimizer, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_result = evaluate_accuracy(testloader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算最终结果\n",
    "result_fin = []\n",
    "for i in range(0,len(res)):\n",
    "    T = 0\n",
    "    F = 0\n",
    "    re = 0\n",
    "    if(res[i] == 1):\n",
    "        T = T+1\n",
    "    else:\n",
    "        F = F+1\n",
    "    if(T1_result[i] == 1):\n",
    "        T = T+1\n",
    "    else:\n",
    "        F = F+1\n",
    "    if(T2_result[i] == 1):\n",
    "        T = T+1\n",
    "    else:\n",
    "        F = F+1\n",
    "    if(T > 1):\n",
    "        re = 1\n",
    "    else:\n",
    "        re = 0\n",
    "    result_fin.append(re)\n",
    "#y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      class1       1.00      1.00      1.00        27\n",
      "      class2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        39\n",
      "   macro avg       1.00      1.00      1.00        39\n",
      "weighted avg       1.00      1.00      1.00        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#指标矩阵\n",
    "from sklearn.metrics import classification_report\n",
    "target = ['class1', 'class2']\n",
    "print(classification_report(y_test, result_fin, target_names=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-418ee40fc55d>:3: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(mean_fpr, fpr, tpr) #对mean_tpr在mean_fpr处进行插值，通过scipy包调用interp()函数\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, result_fin)  \n",
    "mean_tpr += interp(mean_fpr, fpr, tpr) #对mean_tpr在mean_fpr处进行插值，通过scipy包调用interp()函数  \n",
    "mean_tpr[0] = 0.0  #初始处为0  \n",
    "roc_auc = auc(fpr, tpr)\n",
    "#画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数能计算出来  \n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc)) \n",
    "plt.xlim([-0.05, 1.05])  \n",
    "plt.ylim([-0.05, 1.05])  \n",
    "plt.xlabel('False Positive Rate')  \n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('OUR')  \n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
