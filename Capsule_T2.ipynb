{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(inputs, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param inputs: vectors to be squashed\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same size as inputs\n",
    "    \"\"\"\n",
    "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
    "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
    "    return scale * inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Conv2D with `out_channels` and then reshape to get capsules\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param dim_caps: dimension of capsule\n",
    "    :param kernel_size: kernel size\n",
    "    :return: output tensor, size=[batch, num_caps, dim_caps]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
    "        super(PrimaryCapsule, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.conv2d(x)\n",
    "        outputs = outputs.view(x.size(0), -1, self.dim_caps)\n",
    "        return squash(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    The dense capsule layer. It is similar to Dense (FC) layer. Dense layer has `in_num` inputs, each is a scalar, the\n",
    "    output of the neuron from the former layer, and it has `out_num` output neurons. DenseCapsule just expands the\n",
    "    output of the neuron from scalar to vector. So its input size = [None, in_num_caps, in_dim_caps] and output size = \\\n",
    "    [None, out_num_caps, out_dim_caps]. For Dense Layer, in_dim_caps = out_dim_caps = 1.\n",
    "\n",
    "    :param in_num_caps: number of cpasules inputted to this layer\n",
    "    :param in_dim_caps: dimension of input capsules\n",
    "    :param out_num_caps: number of capsules outputted from this layer\n",
    "    :param out_dim_caps: dimension of output capsules\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
    "        super(DenseCapsule, self).__init__()\n",
    "        self.in_num_caps = in_num_caps\n",
    "        self.in_dim_caps = in_dim_caps\n",
    "        self.out_num_caps = out_num_caps\n",
    "        self.out_dim_caps = out_dim_caps\n",
    "        self.routings = routings\n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = torch.squeeze(torch.matmul(self.weight, x[:, None, :, :, None]), dim=-1)\n",
    "        x_hat_detached = x_hat.detach()\n",
    "\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.size = [batch, out_num_caps, in_num_caps]\n",
    "        b = torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)\n",
    "\n",
    "        assert self.routings > 0, 'The \\'routings\\' should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.size = [batch, out_num_caps, in_num_caps]\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `x_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.routings - 1:\n",
    "                # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
    "                # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => outputs.size=   [batch, out_num_caps, 1,           out_dim_caps]\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat))  # alternative way\n",
    "            else:  # Otherwise, use `x_hat_detached` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat_detached))  # alternative way\n",
    "\n",
    "                # outputs.size       =[batch, out_num_caps, 1,           out_dim_caps]\n",
    "                # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => b.size          =[batch, out_num_caps, in_num_caps]\n",
    "                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n",
    "\n",
    "        return torch.squeeze(outputs, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_size: data size = [channels, width, height]\n",
    "    :param classes: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    Shape:\n",
    "        - Input: (batch, channels, width, height), optional (batch, classes) .\n",
    "        - Output:((batch, classes), (batch, channels, width, height))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, classes, routings):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.routings = routings\n",
    "\n",
    "        # Layer 1: Just a conventional Conv2D layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_size[0], 256, kernel_size=9, stride=1, padding=0)\n",
    "\n",
    "        # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps]\n",
    "        self.primarycaps = PrimaryCapsule(\n",
    "            256, 256, 8, kernel_size=9, stride=2, padding=0)\n",
    "\n",
    "        # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "        #self.digitcaps = DenseCapsule(in_num_caps=32*6*6, in_dim_caps=8,\n",
    "        #                              out_num_caps=classes, out_dim_caps=16, routings=routings)\n",
    "        self.digitcaps = DenseCapsule(in_num_caps=1486848, in_dim_caps=8,\n",
    "                                      out_num_caps=classes, out_dim_caps=20, routings=routings)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(40,10)\n",
    "        self.fc2 = nn.Linear(10,2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primarycaps(x)\n",
    "        x = self.digitcaps(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=2)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #length = x.norm(dim=-1)\n",
    "        #print(length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: \n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, optimizer, num_epochs):\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            #print(y_hat)\n",
    "            #print(y)\n",
    "            y = y.long()\n",
    "            l = loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(\n",
    "            'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "               test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"数据标注.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findname(name):\n",
    "    for i in range(0,len(name)):\n",
    "        #print(name[i][0])\n",
    "        if(name[i][0] == '2' and name[i][1] == '-'):\n",
    "            for i in range(0,len(name)):\n",
    "                if(name[i][0] == '3' and name[i][1] == '-'):\n",
    "                    file_dir1 = file_dir+\"/\"+name[i]\n",
    "                    break\n",
    "            break\n",
    "        else:\n",
    "            if(name[i][0] == '5'and name[i][1] == '-'):\n",
    "                for i in range(0,len(name)):\n",
    "                    if(name[i][0] == '6' and name[i][1] == '-'):\n",
    "                        file_dir1 = file_dir+\"/\"+name[i]\n",
    "                        break\n",
    "                break\n",
    "            else:\n",
    "                if(name[i][0] == '6'and name[i][1] == '-'):\n",
    "                    for i in range(0,len(name)):\n",
    "                        if(name[i][0] == '7' and name[i][1] == '-'):\n",
    "                            file_dir1 = file_dir+\"/\"+name[i]\n",
    "                            break\n",
    "                    break\n",
    "                else:\n",
    "                    if(name[i][0] == '3'and name[i][1] == '-'):\n",
    "                        for i in range(0,len(name)):\n",
    "                            if(name[i][0] == '4' and name[i][1] == '-'):\n",
    "                                file_dir1 = file_dir+\"/\"+name[i]\n",
    "                                break\n",
    "                        break\n",
    "                    else:\n",
    "                        if(name[i][0] == '9'and name[i][1] == '-'):\n",
    "                            for i in range(0,len(name)):\n",
    "                                if(name[i][0] == '1' and name[i][1] == '0'):\n",
    "                                    file_dir1 = file_dir+\"/\"+name[i]\n",
    "                                    break\n",
    "                            break\n",
    "    return file_dir1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_n = 100\n",
    "def readimage(file_dir,name,size_n):\n",
    "    character_address = file_dir+\"/\"+name\n",
    "    #print(character_address)\n",
    "    d = cv2.imread(character_address,0)\n",
    "    d = cv2.resize(d, (size_n, size_n))\n",
    "    return d\n",
    "#d = readimage(file_dir1,name_1[0],size_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MR201802210163-Wu HuaHao\n",
      "./MR201802210163-Wu HuaHao/3-20220517144717\n",
      "./MR201706120215-LiangYuQing\n",
      "./MR201706120215-LiangYuQing/3-20220517145225\n",
      "./MR201602290243-Liang QiPan\n",
      "./MR201602290243-Liang QiPan/6-20220517145640\n",
      "./MR201803090487-HuangTianDi\n",
      "./MR201803090487-HuangTianDi/3-20220517145925\n",
      "./MR201710230147-HuangJiangQuan\n",
      "./MR201710230147-HuangJiangQuan/6-20220517150107\n",
      "./MR2017062901390-LinGen You\n",
      "./MR2017062901390-LinGen You/7-20220517150233\n",
      "./MR201707060266-ChenGuiYing\n",
      "./MR201707060266-ChenGuiYing/3-20220517150336\n",
      "./MR201806250211-HuangHuiZhong\n",
      "./MR201806250211-HuangHuiZhong/6-20220517150535\n",
      "./MR201806040279-Li JinRong\n",
      "./MR201806040279-Li JinRong/6-20220517150715\n",
      "./MR202103160080-Chen LianDing\n",
      "./MR202103160080-Chen LianDing/6-20220517150922\n",
      "./MR201807080094-He Zhong\n",
      "./MR201807080094-He Zhong/6-20220517151205\n",
      "./MR201708080222-Li GuiFeng\n",
      "./MR201708080222-Li GuiFeng/4-20220517151335\n",
      "./MR202109160378-Liang YouDi\n",
      "./MR202109160378-Liang YouDi/3-20220517151503\n",
      "./MR201802200016-ZhaoDongSheng\n",
      "./MR201802200016-ZhaoDongSheng/3-20220517151805\n",
      "./MR201803140073-Fang LiChang\n",
      "./MR201803140073-Fang LiChang/3-20220517151953\n",
      "./MR201711160167-Huang ZhiRong\n",
      "./MR201711160167-Huang ZhiRong/6-20220517152111\n",
      "./MR201709170104-LiangSuZhen\n",
      "./MR201709170104-LiangSuZhen/6-20220517152225\n",
      "./MR202110280279-Yang XiTian\n",
      "./MR202110280279-Yang XiTian/6-20220517152610\n",
      "./MR201803170300-Huang GenFa\n",
      "./MR201803170300-Huang GenFa/3-20220517152808\n",
      "./MR201805060354-XieShengYu\n",
      "./MR201805060354-XieShengYu/6-20220517153112\n",
      "./MR201812090066-Lin Nong\n",
      "./MR201812090066-Lin Nong/6-20220517153220\n",
      "./MR201810030079-Huang QiDing\n",
      "./MR201810030079-Huang QiDing/3-20220517153333\n",
      "./MR201806070184-ZhouLiBo\n",
      "./MR201806070184-ZhouLiBo/3-20220517153515\n",
      "./MR201811040118-Pan YiYou\n",
      "./MR201811040118-Pan YiYou/3-20220517153725\n",
      "./MR202204270343-Huang BingQiang\n",
      "./MR202204270343-Huang BingQiang/7-20220517154047\n",
      "./MR201807130152-Zhang SheXie\n",
      "./MR201807130152-Zhang SheXie/6-20220517154212\n",
      "./MR201901140071-Xiao BaoXi\n",
      "./MR201901140071-Xiao BaoXi/3-20220517154335\n",
      "./MR202007040281-Yin CaiDi\n",
      "./MR202007040281-Yin CaiDi/6-20220517154502\n",
      "./MR201907170393-Ou FuTai\n",
      "./MR201907170393-Ou FuTai/6-20220517154543\n",
      "./MR201808280243-Lin ShiMei\n",
      "./MR201808280243-Lin ShiMei/6-20220517154629\n",
      "./MR201808150286-Huang ZhengLian\n",
      "./MR201808150286-Huang ZhengLian/3-20220517154741\n",
      "./MR201902140144-Zhou RuiQiu\n",
      "./MR201902140144-Zhou RuiQiu/7-20220517155045\n",
      "./MR201905270177-Lu SongMao\n",
      "./MR201905270177-Lu SongMao/6-20220517155156\n",
      "./MR201908300389-Huang YanChun\n",
      "./MR201908300389-Huang YanChun/7-20220517150232\n",
      "./MR201909240217-Huang XianYou\n",
      "./MR201909240217-Huang XianYou/3-20220517150311\n",
      "./MR201809280461-Huang GuiPing\n",
      "./MR201809280461-Huang GuiPing/6-20220517150506\n",
      "./MR201908140375-Huang ShunAn\n",
      "./MR201908140375-Huang ShunAn/6-20220517150720\n",
      "./MR201901010047-Zhao PanShun\n",
      "./MR201901010047-Zhao PanShun/3-20220517150852\n",
      "./MR202203140317-Huang MingZhong\n",
      "./MR202203140317-Huang MingZhong/3-20220517151033\n",
      "./MR201906120420-Zhang YanHua\n",
      "./MR201906120420-Zhang YanHua/3-20220517152645\n",
      "./MR202204250318-Yang NuLan\n",
      "./MR202204250318-Yang NuLan/6-20220517152748\n",
      "./MR201811220371-Kuang JianHua\n",
      "./MR201811220371-Kuang JianHua/3-20220517152854\n",
      "./MR201811010032-Zhao QiSheng\n",
      "./MR201811010032-Zhao QiSheng/3-20220517153012\n",
      "./MR201811170295-Kuang GuoHong\n",
      "./MR201811170295-Kuang GuoHong/6-20220517153145\n",
      "./MR201911230367-Zhou QunDuo\n",
      "./MR201911230367-Zhou QunDuo/6-20220517153336\n",
      "./MR202011170463-Zhou YunNu\n",
      "./MR202011170463-Zhou YunNu/6-20220517153422\n",
      "./MR201812290239-Zhang ZhuoSen\n",
      "./MR201812290239-Zhang ZhuoSen/7-20220517153506\n",
      "./MR201906110314-Song YuCheng\n",
      "./MR201906110314-Song YuCheng/6-20220517153551\n",
      "./MR201901190185-Li DuSheng\n",
      "./MR201901190185-Li DuSheng/3-20220517153628\n",
      "./MR202111040024-Lin GuiMei\n",
      "./MR202111040024-Lin GuiMei/6-20220517153826\n",
      "./MR202004090247-Liang DieRong\n",
      "./MR202004090247-Liang DieRong/6-20220517154107\n",
      "./MR201903030032-Zhou WenXing\n",
      "./MR201903030032-Zhou WenXing/6-20220517154142\n",
      "./MR201910280291-LiLinYang\n",
      "./MR201910280291-LiLinYang/7-20220517154235\n",
      "./MR201905210129-Peng DaZe\n",
      "./MR201905210129-Peng DaZe/6-20220517154320\n",
      "./MR201910010051-Zeng ShunFa\n",
      "./MR201910010051-Zeng ShunFa/3-20220517154355\n",
      "./MR201904300236-He ChangLian\n",
      "./MR201904300236-He ChangLian/6-20220517154448\n",
      "./MR202012140342-Liang MeiShun\n",
      "./MR202012140342-Liang MeiShun/6-20220517154616\n",
      "./MR202103270245-Zhou YunHao\n",
      "./MR202103270245-Zhou YunHao/6-20220517154704\n",
      "./MR201909300333-Wu HuanLin\n",
      "./MR201909300333-Wu HuanLin/3-20220517154744\n",
      "./MR201905280271-Zhang RenDe\n",
      "./MR201905280271-Zhang RenDe/6-20220517154819\n",
      "./MR201911230290-Guo FengMan\n",
      "./MR201911230290-Guo FengMan/7-20220517154859\n",
      "./MR-Pan BaoCai\n",
      "./MR-Pan BaoCai/3-20220415151932\n",
      "./MR201709050086-Mei BenZhou\n",
      "./MR201709050086-Mei BenZhou/3-20220415152123\n",
      "./MR201709060196-XieJian\n",
      "./MR201709060196-XieJian/3-20220415152306\n",
      "./MR201709050212-Hu ShengYin\n",
      "./MR201709050212-Hu ShengYin/3-20220415152458\n",
      "./MR202003070164-He QuanHuan\n",
      "./MR202003070164-He QuanHuan/3-20220415152639\n",
      "./MR201709100131-LiangJinDi\n",
      "./MR201709100131-LiangJinDi/6-20220415152728\n",
      "./MR201709120100-LiangJinYuan\n",
      "./MR201709120100-LiangJinYuan/3-20220415152817\n",
      "./MR201709080242-ZhuYuanTian\n",
      "./MR201709080242-ZhuYuanTian/3-20220415152931\n",
      "./MR201709110124-Zhou JianLe\n",
      "./MR201709110124-Zhou JianLe/3-20220415153011\n",
      "./MR201709150146-He LanZhong\n",
      "./MR201709150146-He LanZhong/3-20220415153059\n",
      "./MR201709180125-ChenShaoLing\n",
      "./MR201709180125-ChenShaoLing/3-20220415153143\n",
      "./MR201709130216-LuNiuXi\n",
      "./MR201709130216-LuNiuXi/7-20220415153216\n",
      "./MR201709180311-LuoSan\n",
      "./MR201709180311-LuoSan/3-20220415153244\n",
      "./MR201709170194-WuChangYang\n",
      "./MR201709170194-WuChangYang/3-20220415153313\n",
      "./MR201709250187-WeiLongXi\n",
      "./MR201709250187-WeiLongXi/6-20220415153540\n",
      "./MR201709250264-Zhang HuanJiao\n",
      "./MR201709250264-Zhang HuanJiao/6-20220415153642\n",
      "./MR201709240112-LiaoSheSheng\n",
      "./MR201709240112-LiaoSheSheng/3-20220415153710\n",
      "./MR201709250212-OuYangLiJuan\n",
      "./MR201709250212-OuYangLiJuan/3-20220415153743\n",
      "./MR201709260229-Zhang MinHui\n",
      "./MR201709260229-Zhang MinHui/3-20220415153859\n",
      "./MR201709290106-Liang DaFu\n",
      "./MR201709290106-Liang DaFu/6-20220415153930\n",
      "./MR201710030170-WangPeiGen\n",
      "./MR201710030170-WangPeiGen/3-20220415154105\n",
      "./MR201609020118-Zhou Wei\n",
      "./MR201609020118-Zhou Wei/6-20220415154333\n",
      "./MR201710080077-ZengGuoCai\n",
      "./MR201710080077-ZengGuoCai/6-20220415154409\n",
      "./MR201710260143-LuZhi\n",
      "./MR201710260143-LuZhi/6-20220415154517\n",
      "./MR201710230298-LiangYunHao\n",
      "./MR201710230298-LiangYunHao/7-20220415154554\n",
      "./MR201710280069-HuangJiHan\n",
      "./MR201710280069-HuangJiHan/3-20220415154625\n",
      "./MR201710290080-ZhuJianSheng\n",
      "./MR201710290080-ZhuJianSheng/3-20220415154717\n",
      "./MR201711010144-ChenZhaoGuang\n",
      "./MR201711010144-ChenZhaoGuang/3-20220415154813\n",
      "./MR201803070303-Mo JinMei\n",
      "./MR201803070303-Mo JinMei/6-20220415154838\n",
      "./MR201711040248-YangFengJiao\n",
      "./MR201711040248-YangFengJiao/4-20220415154916\n",
      "./MR201711090081-ZouSanMei\n",
      "./MR201711090081-ZouSanMei/6-20220415154952\n",
      "./MR201704290123-RenZiFeng\n",
      "./MR201704290123-RenZiFeng/3-20220415155226\n",
      "./MR201711080214-LiangBaiSheng\n",
      "./MR201711080214-LiangBaiSheng/3-20220415155316\n",
      "./MR201710190322-ZhaoDongQiang\n",
      "./MR201710190322-ZhaoDongQiang/6-20220415155436\n",
      "./MR201711070313-Zhao YeQin\n",
      "./MR201711070313-Zhao YeQin/3-20220415155521\n",
      "./MR201811160271-Li ZhaoQun\n",
      "./MR201811160271-Li ZhaoQun/3-20220415155735\n",
      "./MR201711200279-RongGuangNing\n",
      "./MR201711200279-RongGuangNing/6-20220415155903\n",
      "./MR201711220152-He MeiQun\n",
      "./MR201711220152-He MeiQun/3-20220415155927\n",
      "./MR201711240273-ChenSiReng\n",
      "./MR201711240273-ChenSiReng/3-20220415160019\n",
      "./MR201711180209-GuoZhenQian\n",
      "./MR201711180209-GuoZhenQian/3-20220415160100\n",
      "./MR201709100154-Lin ZhenQuan\n",
      "./MR201709100154-Lin ZhenQuan/3-20220415160201\n",
      "./MR201711150153-KuangXingYing\n",
      "./MR201711150153-KuangXingYing/6-20220415160254\n",
      "./MR201711300156-LeFaZi\n",
      "./MR201711300156-LeFaZi/6-20220415160326\n",
      "./MR201712010104-Chen EnHua\n",
      "./MR201712010104-Chen EnHua/3-20220415160357\n",
      "./MR201711290250-YaoYanRong\n",
      "./MR201711290250-YaoYanRong/6-20220415160438\n",
      "./MR201904240211-Wu YouGen\n",
      "./MR201904240211-Wu YouGen/6-20220415160500\n",
      "./MR201910070088-Ou DaYing\n",
      "./MR201910070088-Ou DaYing/7-20220415160538\n",
      "./MR201712010181-Liao LianZhong\n",
      "./MR201712010181-Liao LianZhong/6-20220415160611\n",
      "./MR201712050082-FengZaiYing\n",
      "./MR201712050082-FengZaiYing/6-20220415160634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MR201709240102-Lao NuYong\n",
      "./MR201709240102-Lao NuYong/6-20220415160737\n",
      "./MR201712120209-Kuang YanPing\n",
      "./MR201712120209-Kuang YanPing/3-20220415160808\n",
      "./MR201712170069-ZhangSanDing\n",
      "./MR201712170069-ZhangSanDing/3-20220415160918\n",
      "./MR201712180097-Zhao LeSheng\n",
      "./MR201712180097-Zhao LeSheng/3-20220415160959\n",
      "./MR201712190182-WuXuShan\n",
      "./MR201712190182-WuXuShan/4-20220415161023\n",
      "./MR201712220318-MaGuiXiang\n",
      "./MR201712220318-MaGuiXiang/3-20220415161111\n",
      "./MR201712170079-WuWeiHuan\n",
      "./MR201712170079-WuWeiHuan/3-20220415161142\n",
      "./MR201712210191-DengXiuJuan\n",
      "./MR201712210191-DengXiuJuan/6-20220415161215\n",
      "./MR201712210375-XuQiNu\n",
      "./MR201712210375-XuQiNu/3-20220415161245\n",
      "./MR201712240170-Liu QingXiao\n",
      "./MR201712240170-Liu QingXiao/6-20220415161333\n",
      "./MR201712250442-Luo FuJin\n",
      "./MR201712250442-Luo FuJin/6-20220415161429\n",
      "./MR201712260308-ZhouJunQiang\n",
      "./MR201712260308-ZhouJunQiang/3-20220415161450\n",
      "./MR201712190202-ChenKunZhen\n",
      "./MR201712190202-ChenKunZhen/6-20220415161525\n",
      "./MR201712260400-Liang JinKuan\n",
      "./MR201712260400-Liang JinKuan/6-20220415161549\n",
      "./MR201712220354-LuoHuaJin\n",
      "./MR201712220354-LuoHuaJin/6-20220415161712\n",
      "./MR201712310040-HuangYanZhuo\n",
      "./MR201712310040-HuangYanZhuo/3-20220415161824\n",
      "./MR201712260397-Huang GuoDong\n",
      "./MR201712260397-Huang GuoDong/3-20220415161851\n",
      "./MR201801030282-LiChengQing\n",
      "./MR201801030282-LiChengQing/6-20220415162016\n",
      "./MR201712260409-HuangYingYou\n",
      "./MR201712260409-HuangYingYou/6-20220415162110\n",
      "./MR201801270027-KuangGuoCai\n",
      "./MR201801270027-KuangGuoCai/6-20220415162207\n",
      "./MR201805130099-PanBaoLiang\n",
      "./MR201805130099-PanBaoLiang/3-20220415162240\n",
      "./MR201902120162-Zhang Cong\n",
      "./MR201902120162-Zhang Cong/6-20220415162324\n",
      "./MR201810280134-Rong MeiHuan\n",
      "./MR201810280134-Rong MeiHuan/6-20220415162358\n",
      "./MR201901181085-Hu YunLan\n",
      "./MR201901181085-Hu YunLan/6-20220415162422\n",
      "./MR201712310157-Zhou BaiSen\n",
      "./MR201712310157-Zhou BaiSen/3-20220415162446\n",
      "./MR201608140234-Liang JianRong\n",
      "./MR201608140234-Liang JianRong/6-20220415162602\n",
      "./MR201801130182-XuShengQuan\n",
      "./MR201801130182-XuShengQuan/3-20220415162807\n",
      "./MR201801060242-Wang SuZhen\n",
      "./MR201801060242-Wang SuZhen/6-20220415162832\n",
      "./MR201908220397-Zhang GuoYin\n",
      "./MR201908220397-Zhang GuoYin/3-20220415162855\n",
      "./MR201801070152-ZhuShuiZhen\n",
      "./MR201801070152-ZhuShuiZhen/6-20220415163042\n",
      "./MR201801040143-LiuCuiHua\n",
      "./MR201801040143-LiuCuiHua/3-20220415163119\n",
      "./MR201802100202-KuangLiQin\n",
      "./MR201802100202-KuangLiQin/6-20220415163215\n",
      "./MR-Huang XinPei\n",
      "./MR-Huang XinPei/4-20220415163355\n",
      "./MR201801100184-KuangZhuMing\n",
      "./MR201801100184-KuangZhuMing/3-20220415163459\n",
      "./MR201801250133-GanHongYing\n",
      "./MR201801250133-GanHongYing/3-20220415163628\n",
      "./MR201801220266-ChenJianJun\n",
      "./MR201801220266-ChenJianJun/6-20220415163837\n",
      "./MR201801270108-Kuang ChunMei\n",
      "./MR201801270108-Kuang ChunMei/6-20220415163915\n",
      "./MR201801280085-ZhangChengJing\n",
      "./MR201801280085-ZhangChengJing/6-20220415164049\n",
      "./MR201807100283-WuYuLing\n",
      "./MR201807100283-WuYuLing/6-20220415164124\n",
      "./MR201802180035-ChenGuiQuan\n",
      "./MR201802180035-ChenGuiQuan/3-20220415164158\n",
      "./MR201910260239-Huang BaoSheng\n",
      "./MR201910260239-Huang BaoSheng/6-20220415164441\n",
      "./MR201802060103-YeChangYan\n",
      "./MR201802060103-YeChangYan/3-20220415164545\n",
      "./MR201801260243-DengDongYi\n",
      "./MR201801260243-DengDongYi/10-20220415164634\n",
      "./MR201802010201-HuangChengGang\n",
      "./MR201802010201-HuangChengGang/6-20220415164739\n",
      "./MR201811290275-Zhang MingMin\n",
      "./MR201811290275-Zhang MingMin/6-20220415165030\n",
      "./MR201802120177-Huang ZhongTing\n",
      "./MR201802120177-Huang ZhongTing/3-20220415165258\n",
      "./MR201808150161-Sun Jing\n",
      "./MR201808150161-Sun Jing/3-20220415165438\n",
      "./MR201803010795-HuangNuChang\n",
      "./MR201803010795-HuangNuChang/3-20220415165536\n",
      "./MR201802260271-WangXiuZhi\n",
      "./MR201802260271-WangXiuZhi/3-20220415165701\n",
      "./MR201812140342-Lai ShuiYin\n",
      "./MR201812140342-Lai ShuiYin/3-20220415165726\n",
      "./MR201803080295-Liang JinPei\n",
      "./MR201803080295-Liang JinPei/3-20220415165836\n",
      "./MR201803080324-WuSongHua\n",
      "./MR201803080324-WuSongHua/6-20220415165918\n",
      "./MR201803080017-LiuJian\n",
      "./MR201803080017-LiuJian/3-20220415170126\n",
      "./MR201803120307-Liu XiuLian\n",
      "./MR201803120307-Liu XiuLian/6-20220415170231\n",
      "./MR201811230369-Luo PingMei\n",
      "./MR201811230369-Luo PingMei/6-20220415170310\n",
      "./MR201803080200-HuQunYing\n",
      "./MR201803080200-HuQunYing/3-20220415170331\n",
      "./MR201803120328-ZhangXingLai\n",
      "./MR201803120328-ZhangXingLai/3-20220415170359\n",
      "./MR201803150112-LiangChuanLiu\n",
      "./MR201803150112-LiangChuanLiu/6-20220415170435\n",
      "./MR201803120367-Zhou XiuNong\n",
      "./MR201803120367-Zhou XiuNong/6-20220415170459\n",
      "./MR201803140023-ChenYuHong\n",
      "./MR201803140023-ChenYuHong/3-20220415170529\n",
      "./MR201804220167-FengMei\n",
      "./MR201804220167-FengMei/3-20220415170611\n",
      "./MR202106100062-Zhou XianMei\n",
      "./MR202106100062-Zhou XianMei/3-20220415170655\n",
      "./MR201803300207-LuoXiaNu\n",
      "./MR201803300207-LuoXiaNu/3-20220415170901\n",
      "./MR201812120201-Kuang YuXiong\n",
      "./MR201812120201-Kuang YuXiong/6-20220415170952\n",
      "./MR201803290560-ChenJianFang\n",
      "./MR201803290560-ChenJianFang/3-20220415171019\n",
      "./MR201804040022-Chen NianJin\n",
      "./MR201804040022-Chen NianJin/4-20220415171253\n",
      "./MR201803290614-HuangLe\n",
      "./MR201803290614-HuangLe/3-20220415171322\n",
      "./MR201708110264-LvYueQing\n",
      "./MR201708110264-LvYueQing/6-20220415171401\n",
      "./MR201804130221-He WeiYuan\n",
      "./MR201804130221-He WeiYuan/3-20220415171435\n",
      "./MR201804090321-LiGuangMing\n",
      "./MR201804090321-LiGuangMing/3-20220415171516\n",
      "./MR201804130132-WuChangMei\n",
      "./MR201804130132-WuChangMei/7-20220415171548\n",
      "./MR201808060137-Yang XianYing\n",
      "./MR201808060137-Yang XianYing/6-20220415171819\n",
      "./MR201903120286-Duan ShaoPing\n",
      "./MR201903120286-Duan ShaoPing/6-20220415171848\n",
      "./MR201804140320-HuangYuZhen\n",
      "./MR201804140320-HuangYuZhen/3-20220415171923\n",
      "./MR201804200348-YiJiYuan\n",
      "./MR201804200348-YiJiYuan/6-20220415172002\n",
      "./MR201804170212-ChenWangMing\n",
      "./MR201804170212-ChenWangMing/3-20220415172031\n",
      "./MR201903010218-Lv RuiYing\n",
      "./MR201903010218-Lv RuiYing/3-20220415172105\n",
      "./MR201804230415-LinZhenHong\n",
      "./MR201804230415-LinZhenHong/6-20220415172131\n",
      "./MR201804130045-WangLing\n",
      "./MR201804130045-WangLing/3-20220415172152\n",
      "./MR201804200221-LinXiuRong\n",
      "./MR201804200221-LinXiuRong/6-20220415172224\n",
      "./MR201804210230-WenJingCheng\n",
      "./MR201804210230-WenJingCheng/3-20220415172613\n",
      "./MR201709140208-HuangZuoSong\n",
      "./MR201709140208-HuangZuoSong/4-20220415152857\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "X_data = []\n",
    "path_label=[]\n",
    "for h in range(0,len(data[\"ID\"])):\n",
    "    file_dir='./'+data[\"ID\"][h]\n",
    "    print(file_dir)\n",
    "    # ⽬录下⾯的所有⽂件名\n",
    "    name = os.listdir(file_dir)\n",
    "    file_dir2 = findname(name)\n",
    "    print(file_dir2)\n",
    "    name_1 = os.listdir(file_dir2)\n",
    "    #print(h)\n",
    "    #data.append\n",
    "    #print(name_1)\n",
    "    for i in range(0,20):\n",
    "        if i == 0:\n",
    "            #print(file_dir2)\n",
    "            d = readimage(file_dir2,name_1[i],size_n)\n",
    "        else:\n",
    "            c = readimage(file_dir2,name_1[i],size_n)\n",
    "            d = np.hstack((d,c))\n",
    "    #print(d.shape)\n",
    "    d = np.reshape(d, (400, 500))\n",
    "    X_data.append(d)\n",
    "    path_label.append(data[\"Lable\"][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 400, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_1 = np.array(X_data)\n",
    "X_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1 = X_data_1.reshape(X_data_1.shape[0],1 ,X_data_1.shape[1], X_data_1.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 1, 400, 500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 1, 400, 500)\n",
      "(39, 1, 400, 500)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#categorical_labels = to_categorical(y_test, num_classes=2)\n",
    "#print(categorical_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_1, path_label, test_size=0.2)\n",
    "Xtrain = np.array(X_train)\n",
    "Xtext = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "#y_train = to_categorical(y_train, num_classes=2)\n",
    "#y_test = to_categorical(y_test, num_classes=2)\n",
    "print(Xtrain.shape)\n",
    "print(Xtext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True, num_workers=1)\n",
    "testset = torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.6695, train acc 0.686, test acc 0.667, time 958.6 sec\n",
      "epoch 2, loss 0.3290, train acc 0.686, test acc 0.667, time 958.1 sec\n",
      "epoch 3, loss 0.2169, train acc 0.686, test acc 0.667, time 949.1 sec\n",
      "epoch 4, loss 0.1613, train acc 0.686, test acc 0.667, time 957.3 sec\n",
      "epoch 5, loss 0.1274, train acc 0.686, test acc 0.667, time 959.8 sec\n",
      "epoch 6, loss 0.1033, train acc 0.686, test acc 0.667, time 957.0 sec\n",
      "epoch 7, loss 0.0790, train acc 0.686, test acc 0.667, time 951.2 sec\n",
      "epoch 8, loss 0.0629, train acc 0.686, test acc 0.667, time 955.0 sec\n",
      "epoch 9, loss 0.0519, train acc 0.967, test acc 1.000, time 953.9 sec\n",
      "epoch 10, loss 0.0438, train acc 1.000, test acc 1.000, time 951.8 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size, lr, num_epochs = 5, 0.001, 10\n",
    "# load dat\n",
    "#train_iter, test_iter = load_mnist('./data', download=False, batch_size=batch_size)\n",
    "# define model\n",
    "net = CapsuleNet(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#loss = nn.BCELoss()\n",
    "train(trainloader, testloader, net, loss, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'primarycaps.conv2d.weight', 'primarycaps.conv2d.bias', 'digitcaps.weight', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict().keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch.save(net, './model_image_1/model.pkl')        # 保存整个模型\n",
    "#new_model = torch.load('./model_image_1/model.pkl')   # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_model(save_path, iteration, optimizer, model):\n",
    "    torch.save({'iteration': iteration,\n",
    "                'optimizer_dict': optimizer.state_dict(),\n",
    "                'model_dict': model.state_dict()},\n",
    "                save_path)\n",
    "    print(\"model save success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model(save_name, optimizer, model):\n",
    "    model_data = torch.load(save_name)\n",
    "    model.load_state_dict(model_data['model_dict'])\n",
    "    optimizer.load_state_dict(model_data['optimizer_dict'])\n",
    "    print(\"model load success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save success\n"
     ]
    }
   ],
   "source": [
    "path = \"./model_image_2/model_1.pkl\"\n",
    "save_model(path, num_epochs, optimizer, net)\n",
    "\n",
    "#new_model = CapsuleNet(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "#new_optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#loss = nn.CrossEntropyLoss()\n",
    "#load_model(path, new_optimizer, new_model)\n",
    "#print(new_model.state_dict()['linear.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load success\n"
     ]
    }
   ],
   "source": [
    "path = \"./model_image_2/model_1.pkl\"\n",
    "batch_size, lr, num_epochs = 5, 0.001, 10\n",
    "net = CapsuleNet(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "load_model(path, optimizer, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = evaluate_accuracy(testloader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
