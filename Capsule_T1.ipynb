{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(inputs, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param inputs: vectors to be squashed\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same size as inputs\n",
    "    \"\"\"\n",
    "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
    "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
    "    return scale * inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Conv2D with `out_channels` and then reshape to get capsules\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param dim_caps: dimension of capsule\n",
    "    :param kernel_size: kernel size\n",
    "    :return: output tensor, size=[batch, num_caps, dim_caps]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
    "        super(PrimaryCapsule, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.conv2d(x)\n",
    "        outputs = outputs.view(x.size(0), -1, self.dim_caps)\n",
    "        return squash(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    The dense capsule layer. It is similar to Dense (FC) layer. Dense layer has `in_num` inputs, each is a scalar, the\n",
    "    output of the neuron from the former layer, and it has `out_num` output neurons. DenseCapsule just expands the\n",
    "    output of the neuron from scalar to vector. So its input size = [None, in_num_caps, in_dim_caps] and output size = \\\n",
    "    [None, out_num_caps, out_dim_caps]. For Dense Layer, in_dim_caps = out_dim_caps = 1.\n",
    "\n",
    "    :param in_num_caps: number of cpasules inputted to this layer\n",
    "    :param in_dim_caps: dimension of input capsules\n",
    "    :param out_num_caps: number of capsules outputted from this layer\n",
    "    :param out_dim_caps: dimension of output capsules\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
    "        super(DenseCapsule, self).__init__()\n",
    "        self.in_num_caps = in_num_caps\n",
    "        self.in_dim_caps = in_dim_caps\n",
    "        self.out_num_caps = out_num_caps\n",
    "        self.out_dim_caps = out_dim_caps\n",
    "        self.routings = routings\n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = torch.squeeze(torch.matmul(self.weight, x[:, None, :, :, None]), dim=-1)\n",
    "        x_hat_detached = x_hat.detach()\n",
    "\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.size = [batch, out_num_caps, in_num_caps]\n",
    "        b = torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)\n",
    "\n",
    "        assert self.routings > 0, 'The \\'routings\\' should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.size = [batch, out_num_caps, in_num_caps]\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `x_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.routings - 1:\n",
    "                # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
    "                # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => outputs.size=   [batch, out_num_caps, 1,           out_dim_caps]\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat))  # alternative way\n",
    "            else:  # Otherwise, use `x_hat_detached` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat_detached))  # alternative way\n",
    "\n",
    "                # outputs.size       =[batch, out_num_caps, 1,           out_dim_caps]\n",
    "                # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => b.size          =[batch, out_num_caps, in_num_caps]\n",
    "                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n",
    "\n",
    "        return torch.squeeze(outputs, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_size: data size = [channels, width, height]\n",
    "    :param classes: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    Shape:\n",
    "        - Input: (batch, channels, width, height), optional (batch, classes) .\n",
    "        - Output:((batch, classes), (batch, channels, width, height))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, classes, routings):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.classes = classes\n",
    "        self.routings = routings\n",
    "\n",
    "        # Layer 1: Just a conventional Conv2D layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_size[0], 256, kernel_size=9, stride=1, padding=0)\n",
    "\n",
    "        # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps]\n",
    "        self.primarycaps = PrimaryCapsule(\n",
    "            256, 256, 8, kernel_size=9, stride=2, padding=0)\n",
    "\n",
    "        # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "        #self.digitcaps = DenseCapsule(in_num_caps=32*6*6, in_dim_caps=8,\n",
    "        #                              out_num_caps=classes, out_dim_caps=16, routings=routings)\n",
    "        self.digitcaps = DenseCapsule(in_num_caps=1486848, in_dim_caps=8,\n",
    "                                      out_num_caps=classes, out_dim_caps=25, routings=routings)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(50,2)\n",
    "        #self.fc2 = nn.Linear(10,2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primarycaps(x)\n",
    "        x = self.digitcaps(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=2)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        #x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #length = x.norm(dim=-1)\n",
    "        #print(length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: \n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, optimizer, num_epochs):\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            #print(y_hat)\n",
    "            #print(y)\n",
    "            y = y.long()\n",
    "            l = loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print(\n",
    "            'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "               test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"数据标注.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findname(name):\n",
    "    for i in range(0,len(name)):\n",
    "        #print(name[i][0])\n",
    "        if(name[i][0] == '2' and name[i][1] == '-'):\n",
    "            file_dir1 = file_dir+\"/\"+name[i]\n",
    "            break\n",
    "        else:\n",
    "            if(name[i][0] == '5'and name[i][1] == '-'):\n",
    "                file_dir1 = file_dir+\"/\"+name[i]\n",
    "                #print(file_dir1)\n",
    "                break\n",
    "            else:\n",
    "                if(name[i][0] == '6'and name[i][1] == '-'):\n",
    "                    file_dir1 = file_dir+\"/\"+name[i]\n",
    "                    break\n",
    "                else:\n",
    "                    if(name[i][0] == '3'and name[i][1] == '-'):\n",
    "                        file_dir1 = file_dir+\"/\"+name[i]\n",
    "                        break\n",
    "                    else:\n",
    "                        if(name[i][0] == '9'and name[i][1] == '-'):\n",
    "                            file_dir1 = file_dir+\"/\"+name[i]\n",
    "                            break\n",
    "    return file_dir1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_n = 100\n",
    "def readimage(file_dir,name,size_n):\n",
    "    character_address = file_dir+\"/\"+name\n",
    "    #print(character_address)\n",
    "    d = cv2.imread(character_address,0)\n",
    "    d = cv2.resize(d, (size_n, size_n))\n",
    "    return d\n",
    "#d = readimage(file_dir1,name_1[0],size_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MR201802210163-Wu HuaHao\n",
      "./MR201706120215-LiangYuQing\n",
      "./MR201602290243-Liang QiPan\n",
      "./MR201803090487-HuangTianDi\n",
      "./MR201710230147-HuangJiangQuan\n",
      "./MR2017062901390-LinGen You\n",
      "./MR201707060266-ChenGuiYing\n",
      "./MR201806250211-HuangHuiZhong\n",
      "./MR201806040279-Li JinRong\n",
      "./MR202103160080-Chen LianDing\n",
      "./MR201807080094-He Zhong\n",
      "./MR201708080222-Li GuiFeng\n",
      "./MR202109160378-Liang YouDi\n",
      "./MR201802200016-ZhaoDongSheng\n",
      "./MR201803140073-Fang LiChang\n",
      "./MR201711160167-Huang ZhiRong\n",
      "./MR201709170104-LiangSuZhen\n",
      "./MR202110280279-Yang XiTian\n",
      "./MR201803170300-Huang GenFa\n",
      "./MR201805060354-XieShengYu\n",
      "./MR201812090066-Lin Nong\n",
      "./MR201810030079-Huang QiDing\n",
      "./MR201806070184-ZhouLiBo\n",
      "./MR201811040118-Pan YiYou\n",
      "./MR202204270343-Huang BingQiang\n",
      "./MR201807130152-Zhang SheXie\n",
      "./MR201901140071-Xiao BaoXi\n",
      "./MR202007040281-Yin CaiDi\n",
      "./MR201907170393-Ou FuTai\n",
      "./MR201808280243-Lin ShiMei\n",
      "./MR201808150286-Huang ZhengLian\n",
      "./MR201902140144-Zhou RuiQiu\n",
      "./MR201905270177-Lu SongMao\n",
      "./MR201908300389-Huang YanChun\n",
      "./MR201909240217-Huang XianYou\n",
      "./MR201809280461-Huang GuiPing\n",
      "./MR201908140375-Huang ShunAn\n",
      "./MR201901010047-Zhao PanShun\n",
      "./MR202203140317-Huang MingZhong\n",
      "./MR201906120420-Zhang YanHua\n",
      "./MR202204250318-Yang NuLan\n",
      "./MR201811220371-Kuang JianHua\n",
      "./MR201811010032-Zhao QiSheng\n",
      "./MR201811170295-Kuang GuoHong\n",
      "./MR201911230367-Zhou QunDuo\n",
      "./MR202011170463-Zhou YunNu\n",
      "./MR201812290239-Zhang ZhuoSen\n",
      "./MR201906110314-Song YuCheng\n",
      "./MR201901190185-Li DuSheng\n",
      "./MR202111040024-Lin GuiMei\n",
      "./MR202004090247-Liang DieRong\n",
      "./MR201903030032-Zhou WenXing\n",
      "./MR201910280291-LiLinYang\n",
      "./MR201905210129-Peng DaZe\n",
      "./MR201910010051-Zeng ShunFa\n",
      "./MR201904300236-He ChangLian\n",
      "./MR202012140342-Liang MeiShun\n",
      "./MR202103270245-Zhou YunHao\n",
      "./MR201909300333-Wu HuanLin\n",
      "./MR201905280271-Zhang RenDe\n",
      "./MR201911230290-Guo FengMan\n",
      "./MR-Pan BaoCai\n",
      "./MR201709050086-Mei BenZhou\n",
      "./MR201709060196-XieJian\n",
      "./MR201709050212-Hu ShengYin\n",
      "./MR202003070164-He QuanHuan\n",
      "./MR201709100131-LiangJinDi\n",
      "./MR201709120100-LiangJinYuan\n",
      "./MR201709080242-ZhuYuanTian\n",
      "./MR201709110124-Zhou JianLe\n",
      "./MR201709150146-He LanZhong\n",
      "./MR201709180125-ChenShaoLing\n",
      "./MR201709130216-LuNiuXi\n",
      "./MR201709180311-LuoSan\n",
      "./MR201709170194-WuChangYang\n",
      "./MR201709250187-WeiLongXi\n",
      "./MR201709250264-Zhang HuanJiao\n",
      "./MR201709240112-LiaoSheSheng\n",
      "./MR201709250212-OuYangLiJuan\n",
      "./MR201709260229-Zhang MinHui\n",
      "./MR201709290106-Liang DaFu\n",
      "./MR201710030170-WangPeiGen\n",
      "./MR201609020118-Zhou Wei\n",
      "./MR201710080077-ZengGuoCai\n",
      "./MR201710260143-LuZhi\n",
      "./MR201710230298-LiangYunHao\n",
      "./MR201710280069-HuangJiHan\n",
      "./MR201710290080-ZhuJianSheng\n",
      "./MR201711010144-ChenZhaoGuang\n",
      "./MR201803070303-Mo JinMei\n",
      "./MR201711040248-YangFengJiao\n",
      "./MR201711090081-ZouSanMei\n",
      "./MR201704290123-RenZiFeng\n",
      "./MR201711080214-LiangBaiSheng\n",
      "./MR201710190322-ZhaoDongQiang\n",
      "./MR201711070313-Zhao YeQin\n",
      "./MR201811160271-Li ZhaoQun\n",
      "./MR201711200279-RongGuangNing\n",
      "./MR201711220152-He MeiQun\n",
      "./MR201711240273-ChenSiReng\n",
      "./MR201711180209-GuoZhenQian\n",
      "./MR201709100154-Lin ZhenQuan\n",
      "./MR201711150153-KuangXingYing\n",
      "./MR201711300156-LeFaZi\n",
      "./MR201712010104-Chen EnHua\n",
      "./MR201711290250-YaoYanRong\n",
      "./MR201904240211-Wu YouGen\n",
      "./MR201910070088-Ou DaYing\n",
      "./MR201712010181-Liao LianZhong\n",
      "./MR201712050082-FengZaiYing\n",
      "./MR201709240102-Lao NuYong\n",
      "./MR201712120209-Kuang YanPing\n",
      "./MR201712170069-ZhangSanDing\n",
      "./MR201712180097-Zhao LeSheng\n",
      "./MR201712190182-WuXuShan\n",
      "./MR201712220318-MaGuiXiang\n",
      "./MR201712170079-WuWeiHuan\n",
      "./MR201712210191-DengXiuJuan\n",
      "./MR201712210375-XuQiNu\n",
      "./MR201712240170-Liu QingXiao\n",
      "./MR201712250442-Luo FuJin\n",
      "./MR201712260308-ZhouJunQiang\n",
      "./MR201712190202-ChenKunZhen\n",
      "./MR201712260400-Liang JinKuan\n",
      "./MR201712220354-LuoHuaJin\n",
      "./MR201712310040-HuangYanZhuo\n",
      "./MR201712260397-Huang GuoDong\n",
      "./MR201801030282-LiChengQing\n",
      "./MR201712260409-HuangYingYou\n",
      "./MR201801270027-KuangGuoCai\n",
      "./MR201805130099-PanBaoLiang\n",
      "./MR201902120162-Zhang Cong\n",
      "./MR201810280134-Rong MeiHuan\n",
      "./MR201901181085-Hu YunLan\n",
      "./MR201712310157-Zhou BaiSen\n",
      "./MR201608140234-Liang JianRong\n",
      "./MR201801130182-XuShengQuan\n",
      "./MR201801060242-Wang SuZhen\n",
      "./MR201908220397-Zhang GuoYin\n",
      "./MR201801070152-ZhuShuiZhen\n",
      "./MR201801040143-LiuCuiHua\n",
      "./MR201802100202-KuangLiQin\n",
      "./MR-Huang XinPei\n",
      "./MR201801100184-KuangZhuMing\n",
      "./MR201801250133-GanHongYing\n",
      "./MR201801220266-ChenJianJun\n",
      "./MR201801270108-Kuang ChunMei\n",
      "./MR201801280085-ZhangChengJing\n",
      "./MR201807100283-WuYuLing\n",
      "./MR201802180035-ChenGuiQuan\n",
      "./MR201910260239-Huang BaoSheng\n",
      "./MR201802060103-YeChangYan\n",
      "./MR201801260243-DengDongYi\n",
      "./MR201802010201-HuangChengGang\n",
      "./MR201811290275-Zhang MingMin\n",
      "./MR201802120177-Huang ZhongTing\n",
      "./MR201808150161-Sun Jing\n",
      "./MR201803010795-HuangNuChang\n",
      "./MR201802260271-WangXiuZhi\n",
      "./MR201812140342-Lai ShuiYin\n",
      "./MR201803080295-Liang JinPei\n",
      "./MR201803080324-WuSongHua\n",
      "./MR201803080017-LiuJian\n",
      "./MR201803120307-Liu XiuLian\n",
      "./MR201811230369-Luo PingMei\n",
      "./MR201803080200-HuQunYing\n",
      "./MR201803120328-ZhangXingLai\n",
      "./MR201803150112-LiangChuanLiu\n",
      "./MR201803120367-Zhou XiuNong\n",
      "./MR201803140023-ChenYuHong\n",
      "./MR201804220167-FengMei\n",
      "./MR202106100062-Zhou XianMei\n",
      "./MR201803300207-LuoXiaNu\n",
      "./MR201812120201-Kuang YuXiong\n",
      "./MR201803290560-ChenJianFang\n",
      "./MR201804040022-Chen NianJin\n",
      "./MR201803290614-HuangLe\n",
      "./MR201708110264-LvYueQing\n",
      "./MR201804130221-He WeiYuan\n",
      "./MR201804090321-LiGuangMing\n",
      "./MR201804130132-WuChangMei\n",
      "./MR201808060137-Yang XianYing\n",
      "./MR201903120286-Duan ShaoPing\n",
      "./MR201804140320-HuangYuZhen\n",
      "./MR201804200348-YiJiYuan\n",
      "./MR201804170212-ChenWangMing\n",
      "./MR201903010218-Lv RuiYing\n",
      "./MR201804230415-LinZhenHong\n",
      "./MR201804130045-WangLing\n",
      "./MR201804200221-LinXiuRong\n",
      "./MR201804210230-WenJingCheng\n",
      "./MR201709140208-HuangZuoSong\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "X_data = []\n",
    "path_label=[]\n",
    "for h in range(0,len(data[\"ID\"])):\n",
    "    file_dir='./'+data[\"ID\"][h]\n",
    "    print(file_dir)\n",
    "    # ⽬录下⾯的所有⽂件名\n",
    "    name = os.listdir(file_dir)\n",
    "    file_dir2 = findname(name)\n",
    "    name_1 = os.listdir(file_dir2)\n",
    "    #print(h)\n",
    "    #data.append\n",
    "    #print(name_1)\n",
    "    #print(file_dir2)\n",
    "    for i in range(0,20):\n",
    "        if i == 0:\n",
    "            #print(file_dir2)\n",
    "            d = readimage(file_dir2,name_1[i],size_n)\n",
    "        else:\n",
    "            c = readimage(file_dir2,name_1[i],size_n)\n",
    "            d = np.hstack((d,c))\n",
    "    #print(d.shape)\n",
    "    d = np.reshape(d, (400, 500))\n",
    "    X_data.append(d)\n",
    "    path_label.append(data[\"Lable\"][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 400, 500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_1 = np.array(X_data)\n",
    "X_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_1 = X_data_1.reshape(X_data_1.shape[0],1 ,X_data_1.shape[1], X_data_1.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 1, 400, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 1, 400, 500)\n",
      "(39, 1, 400, 500)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#categorical_labels = to_categorical(y_test, num_classes=2)\n",
    "#print(categorical_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_1, path_label, test_size=0.2)\n",
    "Xtrain = np.array(X_train)\n",
    "Xtext = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "#y_train = to_categorical(y_train, num_classes=2)\n",
    "#y_test = to_categorical(y_test, num_classes=2)\n",
    "print(Xtrain.shape)\n",
    "print(Xtext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True, num_workers=1)\n",
    "testset = torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.6526, train acc 0.673, test acc 0.641, time 1101.7 sec\n",
      "epoch 2, loss 0.3129, train acc 0.693, test acc 0.641, time 1098.4 sec\n",
      "epoch 3, loss 0.1969, train acc 0.739, test acc 1.000, time 1099.3 sec\n",
      "epoch 4, loss 0.1200, train acc 0.993, test acc 1.000, time 1099.8 sec\n",
      "epoch 5, loss 0.0867, train acc 1.000, test acc 1.000, time 1108.0 sec\n",
      "epoch 6, loss 0.0680, train acc 1.000, test acc 1.000, time 1111.1 sec\n",
      "epoch 7, loss 0.0560, train acc 1.000, test acc 1.000, time 1095.6 sec\n",
      "epoch 8, loss 0.0474, train acc 1.000, test acc 1.000, time 1072.0 sec\n",
      "epoch 9, loss 0.0411, train acc 1.000, test acc 1.000, time 1069.7 sec\n",
      "epoch 10, loss 0.0363, train acc 1.000, test acc 1.000, time 1067.8 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size, lr, num_epochs = 5, 0.001, 10\n",
    "# load dat\n",
    "#train_iter, test_iter = load_mnist('./data', download=False, batch_size=batch_size)\n",
    "# define model\n",
    "net = CapsuleNet(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "#loss = nn.BCELoss()\n",
    "train(trainloader, testloader, net, loss, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'primarycaps.conv2d.weight', 'primarycaps.conv2d.bias', 'digitcaps.weight', 'fc1.weight', 'fc1.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict().keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(net, './model_image_1/model.pkl')        # 保存整个模型\n",
    "#new_model = torch.load('./model_image_1/model.pkl')   # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_model(save_path, iteration, optimizer, model):\n",
    "    torch.save({'iteration': iteration,\n",
    "                'optimizer_dict': optimizer.state_dict(),\n",
    "                'model_dict': model.state_dict()},\n",
    "                save_path)\n",
    "    print(\"model save success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model(save_name, optimizer, model):\n",
    "    model_data = torch.load(save_name)\n",
    "    model.load_state_dict(model_data['model_dict'])\n",
    "    optimizer.load_state_dict(model_data['optimizer_dict'])\n",
    "    print(\"model load success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model save success\n"
     ]
    }
   ],
   "source": [
    "path = \"./model_image_1/model_1.pkl\"\n",
    "save_model(path, num_epochs, optimizer, net)\n",
    "\n",
    "#new_model = CapsuleNet(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "#new_optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#loss = nn.CrossEntropyLoss()\n",
    "#load_model(path, new_optimizer, new_model)\n",
    "#print(new_model.state_dict()['linear.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load success\n"
     ]
    }
   ],
   "source": [
    "path = \"./model_image_1/model_1.pkl\"\n",
    "batch_size, lr, num_epochs = 5, 0.001, 10\n",
    "net = CapsuleNet(input_size=[1, 400, 500], classes=2, routings=2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "load_model(path, optimizer, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = evaluate_accuracy(testloader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
